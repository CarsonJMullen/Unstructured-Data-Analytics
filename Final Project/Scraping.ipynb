{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4f7e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install eventregistry\n",
    "# !python -m spacy download en_core_web_lg\n",
    "# from eventregistry import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "# import spacy # type: ignore\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the API key from the JSON file\n",
    "with open(\"config.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "api_key = config[\"api_key\"]\n",
    "\n",
    "# Initialize EventRegistry with the API key\n",
    "er = EventRegistry(apiKey=api_key, allowUseOfArchive=False)\n",
    "\n",
    "# Define topics to search for\n",
    "topics = [\n",
    "    \"Donald Trump\", \"Kamala Harris\", \"Israel\", \"Palestine\", \"Palestinians\", \"Hamas\", \"FEMA\", \"Abortion\",\n",
    "    \"Inflation\", \"Unemployment\", \"Economy\", \"Dockworkers\", \"ILA Port\", \"Immigration\"\n",
    "]\n",
    "\n",
    "# Define sources to search for and get their URIs\n",
    "source_names = [\"NPR\", \"MSNBC\", \"AP News\", \"FOX\", \"Forbes\"]\n",
    "source_uris = {source: er.getNewsSourceUri(source) for source in source_names}\n",
    "\n",
    "# List to store the names of all generated DataFrames\n",
    "dataframe_names = []\n",
    "\n",
    "# Loop through each topic\n",
    "for topic in topics:\n",
    "    # Get the URI for the concept\n",
    "    concept_uri = er.getConceptUri(topic)\n",
    "    \n",
    "    # List to hold all articles' data for the current topic (across all sources)\n",
    "    articles_data = []\n",
    "    \n",
    "    # Loop through each source individually\n",
    "    for source_name, source_uri in source_uris.items():\n",
    "        # Define the query for each topic and source\n",
    "        q = QueryArticlesIter(\n",
    "            conceptUri=concept_uri,\n",
    "            sourceUri=source_uri,\n",
    "            sourceLocationUri=er.getLocationUri(\"United States\"),  # Only US sources\n",
    "        )\n",
    "\n",
    "        # Fetch and accumulate up to 500 articles for the current topic from this source\n",
    "        for art in q.execQuery(er, sortBy=\"date\", maxItems=500):\n",
    "            articles_data.append({\n",
    "                \"title\": art.get(\"title\"),\n",
    "                \"source\": art.get(\"source\", {}).get(\"title\"),\n",
    "                \"author\": art.get(\"author\"),\n",
    "                \"url\": art.get(\"url\"),\n",
    "                \"publishedAt\": art.get(\"dateTime\"),\n",
    "                \"content\": art.get(\"body\")\n",
    "            })\n",
    "\n",
    "    # Create a single DataFrame for the current topic with articles from all sources\n",
    "    articles_df = pd.DataFrame(articles_data)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    file_name = f\"{topic.replace(' ', '_')}_articles.csv\"\n",
    "    articles_df.to_csv(file_name, index=False)\n",
    "\n",
    "    # Dynamically set the DataFrame name based on the topic, replacing spaces with underscores\n",
    "    df_name = f\"{topic.replace(' ', '_')}_df\"\n",
    "    globals()[df_name] = articles_df\n",
    "\n",
    "    # Append the DataFrame name to the list\n",
    "    dataframe_names.append(df_name)\n",
    "\n",
    "# Print the list of all generated DataFrame names\n",
    "print(\"Generated DataFrames:\", dataframe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Access the \"Donald Trump\" DataFrame\n",
    "Donald_Trump_df.head() # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f989d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Donald_Trump_articles.csv')\n",
    "df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c46efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Abortion_articles.csv')\n",
    "df1['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247feac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Merge relevant dataframes - find relevant actors for each --> Mason\n",
    "# 2. Data Cleaning/LDA --> Carson\n",
    "# 2. Active/passive/topic voice code --> Timmy \n",
    "# 3. ChatGPT sentiment for actors in a given category (-100 to 100) --> Ethan/Timmy\n",
    "# 4. Coming up with some sort of logical evaluation score metric based on active/passive voice, sentiment, title, and number of articles about certain subjects etc. --> Bethel/Medha (Come up with a few options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e70657-40d7-4f1a-be52-f5346f4dfe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine related dataframes\n",
    "\n",
    "# Read in csv's\n",
    "# List of topics\n",
    "topics = [\n",
    "    \"Donald Trump\", \"Kamala Harris\", \"Israel\", \"Palestine\", \"Palestinians\", \"Hamas\", \"FEMA\", \"Abortion\",\n",
    "    \"Inflation\", \"Unemployment\", \"Economy\", \"Dockworkers\", \"ILA Port\", \"Immigration\"\n",
    "]\n",
    "\n",
    "# Dictionary to hold the DataFrames after reading them from CSV\n",
    "dataframes = {}\n",
    "\n",
    "# Loop to read each CSV and store the DataFrame in the dictionary\n",
    "for topic in topics:\n",
    "    # Replace spaces with underscores to match your file naming convention\n",
    "    file_name = f\"{topic.replace(' ', '_')}_articles.csv\"\n",
    "    try:\n",
    "        dataframes[topic.replace(' ', '_')] = pd.read_csv(file_name)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")  # If a file is not found\n",
    "\n",
    "# Merge Donald Trump and Kamala Harris\n",
    "Donald_Trump_Kamala_Harris_df = pd.concat([dataframes[\"Donald_Trump\"], dataframes[\"Kamala_Harris\"]])\n",
    "\n",
    "# Merge Israel, Palestine, Palestinians, and Hamas\n",
    "Israel_Palestine_Palestinians_Hamas_df = pd.concat([\n",
    "    dataframes[\"Israel\"], dataframes[\"Palestine\"], dataframes[\"Palestinians\"], dataframes[\"Hamas\"]\n",
    "])\n",
    "\n",
    "# Merge Inflation, Unemployment, and Economy\n",
    "Inflation_Unemployment_Economy_df = pd.concat([\n",
    "    dataframes[\"Inflation\"], dataframes[\"Unemployment\"], dataframes[\"Economy\"]\n",
    "])\n",
    "\n",
    "# Merge Dockworkers and ILA Port\n",
    "Dockworkers_ILA_Port_df = pd.concat([\n",
    "    dataframes[\"Dockworkers\"], dataframes[\"ILA_Port\"]\n",
    "])\n",
    "\n",
    "print(Donald_Trump_Kamala_Harris_df.head())\n",
    "print(Israel_Palestine_Palestinians_Hamas_df.head())\n",
    "print(Inflation_Unemployment_Economy_df.head())\n",
    "print(Dockworkers_ILA_Port_df.head())\n",
    "\n",
    "# Add the merged dfs to dataframes\n",
    "dataframes[\"Donald_Trump_Kamala_Harris\"] = Donald_Trump_Kamala_Harris_df\n",
    "dataframes[\"Israel_Palestine_Palestinians_Hamas\"] = Israel_Palestine_Palestinians_Hamas_df\n",
    "dataframes[\"Inflation_Unemployment_Economy\"] = Inflation_Unemployment_Economy_df\n",
    "dataframes[\"Dockworkers_ILA_Port\"] = Dockworkers_ILA_Port_df\n",
    "\n",
    "# Remove topics that became merged\n",
    "del_keys = [\"Donald_Trump\", \"Kamala_Harris\", \"Israel\", \"Palestine\", \"Palestinians\", \"Hamas\",\n",
    "            \"Inflation\", \"Unemployment\", \"Economy\", \"Dockworkers\", \"ILA_Port\"]\n",
    "for key in del_keys:\n",
    "    del dataframes[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a08b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentences(text):\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    return len([s for s in sentences if s.strip()])\n",
    "\n",
    "def clean_df(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df['content'] = df['content'].str.replace(\"By entering your email and pushing continue, you are agreeing to Fox News\\' Terms of Use and Privacy Policy, which includes our Notice of Financial Incentive.\\n\\n\", \"\")\n",
    "    df['num_sentences'] = df['content'].apply(count_sentences)\n",
    "\n",
    "    return df\n",
    "\n",
    "clean_donald_kamala = clean_df(Donald_Trump_Kamala_Harris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b749c-7eb8-4d7b-826a-94d2e347d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Function to extract nouns and proper nouns\n",
    "def extract_nouns(text):\n",
    "    if pd.isnull(text):  # Handle missing values\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    nouns = [token.text for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"] and token.is_alpha and len(token.text) > 2]\n",
    "    return nouns\n",
    "\n",
    "# Loop through all DataFrames\n",
    "for topic, df in dataframes.items():\n",
    "    print(f\"Processing topic: {topic}\")\n",
    "    \n",
    "    # Apply the noun extraction to both the 'title' and 'content' columns\n",
    "    df['Title_Nouns'] = df['title'].apply(extract_nouns)\n",
    "    df['Content_Nouns'] = df['content'].apply(extract_nouns)\n",
    "\n",
    "    # Combine all noun lists into one for counting\n",
    "    all_nouns = df['Title_Nouns'].sum() + df['Content_Nouns'].sum()  # Flatten lists so that the noun lists from both the Title_Nouns and Content_Nouns columns are in a single list\n",
    "    \n",
    "    # Calculate the value counts\n",
    "    noun_counts = pd.Series(all_nouns).value_counts()\n",
    "    \n",
    "    # Output the top 10 most frequent nouns\n",
    "    print(f\"Top nouns for {topic}:\\n\", noun_counts.head(10))\n",
    "\n",
    "    # Store the counts for further analysis\n",
    "    dataframes[topic]['Noun_Counts'] = noun_counts\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
