{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b1dc68",
   "metadata": {},
   "source": [
    "**Members:** Ethan Wong, Timmy Ren, Mason Shu, Medha Nalamada, Carson Mullen, Bethel Kim\n",
    "\n",
    "**Morning Cohort**: 11 AM - 1 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7563a96",
   "metadata": {},
   "source": [
    "**Current Tasks Remaining:**\n",
    "\n",
    "1. Verify Relevant Actors Code --> Timmy\n",
    "2. Perform LDA --> Mason\n",
    "3. Perform Active/Passive/Topic Voice Analysis --> Timmy\n",
    "4. Use ChatGPT to generate sentiment for actors in a given category (-100 to 100) --> Ethan\n",
    "5. Create a logical evaluation score metric based on active/passive voice, sentiment, title, and number of articles about certain subjects etc.(Come up with a few options) --> Bethel and Medha (Timmy and Mason will talk to Barua)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb917e6",
   "metadata": {},
   "source": [
    "# Install and Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f7e67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 2.4/12.8 MB 12.2 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 13.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 22.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 19.6 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#!pip install eventregistry\n",
    "!python -m spacy download en_core_web_lg\n",
    "#!python -m spacy download en_core_web_sm\n",
    "from eventregistry import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import spacy # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928fbe1",
   "metadata": {},
   "source": [
    "# Scraping Articles with Event Registry API - Ethan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcaf719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated DataFrames: ['Election_df', 'Gaza_df', 'FEMA_df', 'Abortion_df', 'Inflation_df', 'Unemployment_df', 'Dockworkers_df', 'Immigration_df']\n"
     ]
    }
   ],
   "source": [
    "# Load the API key from the JSON file\n",
    "with open(\"config.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "api_key = config[\"api_key\"]\n",
    "\n",
    "# Initialize EventRegistry with the API key\n",
    "er = EventRegistry(apiKey=api_key, allowUseOfArchive=False)\n",
    "\n",
    "# Define topics to search for\n",
    "#topics = [\n",
    "    #\"Donald Trump\", \"Kamala Harris\", \"Israel\", \"Palestine\", \"Palestinians\", \"Hamas\", \"FEMA\", \"Abortion\",\n",
    "    #\"Inflation\", \"Unemployment\", \"Economy\", \"Dockworkers\", \"ILA Port\", \"Immigration\"\n",
    "#]\n",
    "\n",
    "topics = [\"Election\", \"Gaza\", \"FEMA\", \"Abortion\", \"Inflation\", \"Unemployment\", \"Dockworkers\", \"Immigration\"]\n",
    "\n",
    "# Define sources to search for and get their URIs\n",
    "source_names = [\"NPR\", \"MSNBC\", \"AP News\", \"FOX\", \"Forbes\"]\n",
    "source_uris = {source: er.getNewsSourceUri(source) for source in source_names}\n",
    "\n",
    "# List to store the names of all generated DataFrames\n",
    "dataframe_names = []\n",
    "\n",
    "# Loop through each topic\n",
    "for topic in topics:\n",
    "    # Get the URI for the concept\n",
    "    concept_uri = er.getConceptUri(topic)\n",
    "    \n",
    "    # List to hold all articles' data for the current topic (across all sources)\n",
    "    articles_data = []\n",
    "    \n",
    "    # Loop through each source individually\n",
    "    for source_name, source_uri in source_uris.items():\n",
    "        # Define the query for each topic and source\n",
    "        q = QueryArticlesIter(\n",
    "            conceptUri=concept_uri,\n",
    "            sourceUri=source_uri,\n",
    "            sourceLocationUri=er.getLocationUri(\"United States\"),  # Only US sources\n",
    "        )\n",
    "\n",
    "        # Fetch and accumulate up to 500 articles for the current topic from this source\n",
    "        for art in q.execQuery(er, sortBy=\"date\", maxItems=500):\n",
    "            articles_data.append({\n",
    "                \"title\": art.get(\"title\"),\n",
    "                \"source\": art.get(\"source\", {}).get(\"title\"),\n",
    "                \"author\": art.get(\"author\"),\n",
    "                \"url\": art.get(\"url\"),\n",
    "                \"publishedAt\": art.get(\"dateTime\"),\n",
    "                \"content\": art.get(\"body\")\n",
    "            })\n",
    "\n",
    "    # Create a single DataFrame for the current topic with articles from all sources\n",
    "    articles_df = pd.DataFrame(articles_data)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    file_name = f\"{topic.replace(' ', '_')}_articles.csv\"\n",
    "    articles_df.to_csv(file_name, index=False)\n",
    "\n",
    "    # Dynamically set the DataFrame name based on the topic, replacing spaces with underscores\n",
    "    df_name = f\"{topic.replace(' ', '_')}_df\"\n",
    "    globals()[df_name] = articles_df\n",
    "\n",
    "    # Append the DataFrame name to the list\n",
    "    dataframe_names.append(df_name)\n",
    "\n",
    "# Print the list of all generated DataFrame names\n",
    "print(\"Generated DataFrames:\", dataframe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1055b4b",
   "metadata": {},
   "source": [
    "# Cleaning Dataframes and Running Named Entity Recognition - Mason and Carson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3f3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in CSVs dynamically\n",
    "\n",
    "topics = [\"Election\", \"Gaza\", \"FEMA\", \"Abortion\", \"Inflation\", \"Unemployment\", \"Dockworkers\", \"Immigration\"]\n",
    "\n",
    "# Dictionary to hold the DataFrames after reading them from CSV\n",
    "dataframes = {}\n",
    "\n",
    "# Loop to read each CSV and store the DataFrame in the dictionary\n",
    "for topic in topics:\n",
    "    # Replace spaces with underscores to match your file naming convention\n",
    "    file_name = f\"{topic.replace(' ', '_')}_articles.csv\"\n",
    "    try:\n",
    "        dataframes[topic.replace(' ', '_')] = pd.read_csv(file_name)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")  # If a file is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46875a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframes\n",
    "def count_sentences(text):\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    return len([s for s in sentences if s.strip()])\n",
    "\n",
    "def clean_df(df):\n",
    "    df = df.drop_duplicates().copy()\n",
    "    df.loc[:, 'content'] = df['content'].str.replace(\n",
    "        \"By entering your email and pushing continue, you are agreeing to Fox News\\' Terms of Use and Privacy Policy, which includes our Notice of Financial Incentive.\\n\\n\", \n",
    "        \"\", \n",
    "        regex=False\n",
    "    )\n",
    "    df.loc[:, 'num_sentences'] = df['content'].apply(count_sentences)\n",
    "    return df\n",
    "\n",
    "# Apply clean_df to each DataFrame in the dictionary\n",
    "cleaned_dataframes = {key: clean_df(df) for key, df in dataframes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mason\n",
    "\n",
    "# Finding relevant actors with noun extraction\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Function to extract nouns and proper nouns\n",
    "def extract_nouns(text):\n",
    "    if pd.isnull(text):  # Handle missing values\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    nouns = [token.text for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"] and token.is_alpha and len(token.text) > 2]\n",
    "    return nouns\n",
    "\n",
    "# Loop through all DataFrames\n",
    "for topic, df in cleaned_dataframes.items():\n",
    "    print(f\"Processing topic: {topic}\")\n",
    "    \n",
    "    # Apply the noun extraction to both the 'title' and 'content' columns\n",
    "    df['Title_Nouns'] = df['title'].apply(extract_nouns)\n",
    "    df['Content_Nouns'] = df['content'].apply(extract_nouns)\n",
    "\n",
    "    # Combine all noun lists into one for counting\n",
    "    all_nouns = df['Title_Nouns'].sum() + df['Content_Nouns'].sum()  # Flatten lists so that the noun lists from both the Title_Nouns and Content_Nouns columns are in a single list\n",
    "    \n",
    "    # Calculate the value counts\n",
    "    noun_counts = pd.Series(all_nouns).value_counts()\n",
    "    \n",
    "    # Output the top 10 most frequent nouns\n",
    "    print(f\"Top nouns for {topic}:\\n\", noun_counts.head(10))\n",
    "\n",
    "    # Store the counts for further analysis\n",
    "    cleaned_dataframes[topic]['Noun_Counts'] = noun_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4112c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: Election\n",
      "Top entities for Election:\n",
      " Trump           7611\n",
      "Harris          4119\n",
      "Biden           1937\n",
      "U.S.            1871\n",
      "Republican      1760\n",
      "first           1541\n",
      "Democrats       1338\n",
      "one             1266\n",
      "Republicans     1254\n",
      "Donald Trump    1236\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Gaza\n",
      "Top entities for Gaza:\n",
      " Israel       3739\n",
      "Hezbollah    1731\n",
      "Israeli      1514\n",
      "Gaza         1339\n",
      "Lebanon      1300\n",
      "Hamas        1143\n",
      "Iran          875\n",
      "U.S.          820\n",
      "Biden         510\n",
      "Harris        459\n",
      "Name: count, dtype: int64\n",
      "Processing topic: FEMA\n",
      "Top entities for FEMA:\n",
      " first               1718\n",
      "Trump               1368\n",
      "one                 1245\n",
      "Harris              1180\n",
      "two                 1130\n",
      "three                621\n",
      "U.S.                 611\n",
      "AP                   591\n",
      "Fox News Digital     534\n",
      "second               393\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Abortion\n",
      "Top entities for Abortion:\n",
      " Trump           2790\n",
      "Harris          1889\n",
      "Republican       638\n",
      "Vance            610\n",
      "Democrats        610\n",
      "Biden            537\n",
      "Republicans      504\n",
      "first            489\n",
      "Democratic       438\n",
      "Donald Trump     422\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Inflation\n",
      "Top entities for Inflation:\n",
      " Trump        1766\n",
      "Harris       1316\n",
      "U.S.          994\n",
      "Fed           965\n",
      "first         659\n",
      "Biden         553\n",
      "one           456\n",
      "Americans     368\n",
      "today         367\n",
      "2024          350\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Unemployment\n",
      "Top entities for Unemployment:\n",
      " Fed          473\n",
      "Trump        238\n",
      "U.S.         235\n",
      "Harris       182\n",
      "first        150\n",
      "one          134\n",
      "AI           133\n",
      "China        111\n",
      "September    108\n",
      "Americans    100\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Dockworkers\n",
      "Top entities for Dockworkers:\n",
      " Trump         3965\n",
      "first         1972\n",
      "Harris        1753\n",
      "one           1613\n",
      "two           1590\n",
      "U.S.          1271\n",
      "Israel        1122\n",
      "Republican    1014\n",
      "Biden          876\n",
      "three          792\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Immigration\n",
      "Top entities for Immigration:\n",
      " Trump           3941\n",
      "Harris          2295\n",
      "U.S.            1120\n",
      "Biden            963\n",
      "Republican       779\n",
      "Vance            778\n",
      "first            755\n",
      "Springfield      672\n",
      "Donald Trump     615\n",
      "one              610\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Mason \n",
    "\n",
    "# NER, to find the major parties in each topic\n",
    "\n",
    "# Initialize the NLP model \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Function to extract all named entities from a list of texts\n",
    "def extract_entities_batch(texts):\n",
    "    docs = nlp.pipe(texts, batch_size=50)  # Adjust batch_size as needed\n",
    "    entities_list = []\n",
    "    for doc in docs:\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]  # Get all entities and their types\n",
    "        entities_list.append(entities)\n",
    "    return entities_list\n",
    "\n",
    "# Loop through all DataFrames\n",
    "for topic, df in cleaned_dataframes.items():\n",
    "    print(f\"Processing topic: {topic}\")\n",
    "    \n",
    "    # Apply the NER to the 'title' and 'content' columns using batch processing\n",
    "    df['Title_Entities'] = extract_entities_batch(df['title'].tolist())\n",
    "    df['Content_Entities'] = extract_entities_batch(df['content'].tolist())\n",
    "\n",
    "    # Combine all entity lists into one for counting\n",
    "    all_entities = df['Title_Entities'].sum() + df['Content_Entities'].sum()  # Combine title and content\n",
    "    \n",
    "    # Calculate the value counts\n",
    "    entity_texts = [ent[0] for ent in all_entities]  # Extract just the text of the entities\n",
    "    entity_counts = pd.Series(entity_texts).value_counts()\n",
    "    \n",
    "    # Output the top 10 most frequent entities\n",
    "    print(f\"Top entities for {topic}:\\n\", entity_counts.head(10))\n",
    "\n",
    "    # Store the entity counts\n",
    "    cleaned_dataframes[topic]['Entity_Counts'] = entity_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2127e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: Election\n",
      "Top entities for Election:\n",
      " Trump           7078\n",
      "Harris          4119\n",
      "Biden           1937\n",
      "Vance            963\n",
      "GOP              826\n",
      "Georgia          787\n",
      "Congress         659\n",
      "Walz             654\n",
      "Pennsylvania     638\n",
      "Israel           618\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Gaza\n",
      "Top entities for Gaza:\n",
      " Israel       3739\n",
      "Hezbollah    1708\n",
      "Gaza         1339\n",
      "Lebanon      1300\n",
      "Hamas        1143\n",
      "Iran          875\n",
      "Biden         510\n",
      "Harris        459\n",
      "Trump         411\n",
      "Beirut        288\n",
      "Name: count, dtype: int64\n",
      "Processing topic: FEMA\n",
      "Top entities for FEMA:\n",
      " Trump         1237\n",
      "Harris        1180\n",
      "Georgia        357\n",
      "Biden          355\n",
      "California     294\n",
      "Combs          293\n",
      "Texas          282\n",
      "Israel         274\n",
      "America        253\n",
      "Florida        234\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Abortion\n",
      "Top entities for Abortion:\n",
      " Trump       2572\n",
      "Harris      1889\n",
      "Vance        610\n",
      "Biden        537\n",
      "Walz         387\n",
      "GOP          315\n",
      "Georgia      295\n",
      "Senate       258\n",
      "Robinson     215\n",
      "Florida      210\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Inflation\n",
      "Top entities for Inflation:\n",
      " Trump       1648\n",
      "Harris      1316\n",
      "Fed          965\n",
      "Biden        553\n",
      "China        291\n",
      "Forbes       279\n",
      "America      196\n",
      "Israel       190\n",
      "Vance        181\n",
      "Michigan     175\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Unemployment\n",
      "Top entities for Unemployment:\n",
      " Fed       473\n",
      "Trump     221\n",
      "Harris    182\n",
      "China     111\n",
      "FOMC       98\n",
      "Biden      95\n",
      "Forbes     88\n",
      "Israel     60\n",
      "Gaza       51\n",
      "Powell     41\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Dockworkers\n",
      "Top entities for Dockworkers:\n",
      " Trump        3669\n",
      "Harris       1753\n",
      "Israel       1122\n",
      "Biden         876\n",
      "Florida       758\n",
      "Vance         586\n",
      "GOP           549\n",
      "Hezbollah     474\n",
      "Lebanon       453\n",
      "NFL           426\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Immigration\n",
      "Top entities for Immigration:\n",
      " Trump          3669\n",
      "Harris         2295\n",
      "Biden           963\n",
      "Vance           778\n",
      "Springfield     672\n",
      "Ohio            595\n",
      "GOP             367\n",
      "Walz            356\n",
      "America         313\n",
      "Congress        286\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Timmy\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Function to extract named entities from a document\n",
    "def extract_entities_from_docs(docs):\n",
    "    entity_lists = []\n",
    "    for doc in docs:\n",
    "        entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"LOC\"] and ent.text.isalpha() and len(ent.text) > 2]\n",
    "        entity_lists.append(entities)\n",
    "    return entity_lists\n",
    "\n",
    "# Loop through all DataFrames\n",
    "for topic, df in cleaned_dataframes.items():\n",
    "    print(f\"Processing topic: {topic}\")\n",
    "    \n",
    "    # Filter out rows with missing data in 'title' and 'content'\n",
    "    df = df.dropna(subset=['title', 'content'])\n",
    "\n",
    "    # nlp.pipe is btch processing\n",
    "    title_docs = nlp.pipe(df['title'], disable=[\"textcat\"])\n",
    "    content_docs = nlp.pipe(df['content'], disable=[\"textcat\"])\n",
    "    \n",
    "    df['Title_Entities'] = extract_entities_from_docs(title_docs)\n",
    "    df['Content_Entities'] = extract_entities_from_docs(content_docs)\n",
    "\n",
    "    # Combine all entity lists into one for counting\n",
    "    all_entities = df['Title_Entities'].sum() + df['Content_Entities'].sum()\n",
    "    \n",
    "    # Calculate the value counts\n",
    "    entity_counts = pd.Series(all_entities).value_counts()\n",
    "    \n",
    "    # Output the top 10 most frequent entities\n",
    "    print(f\"Top entities for {topic}:\\n\", entity_counts.head(10))\n",
    "\n",
    "    # Store the counts for further analysis\n",
    "    cleaned_dataframes[topic]['Entity_Counts'] = entity_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2d51e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: Election\n",
      "Top entities for Election:\n",
      " Trump           7078\n",
      "Harris          4119\n",
      "Biden           1937\n",
      "Vance            963\n",
      "GOP              826\n",
      "Georgia          787\n",
      "Congress         659\n",
      "Walz             654\n",
      "Pennsylvania     638\n",
      "Israel           618\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Gaza\n",
      "Top entities for Gaza:\n",
      " Israel       3739\n",
      "Hezbollah    1708\n",
      "Gaza         1339\n",
      "Lebanon      1300\n",
      "Hamas        1143\n",
      "Iran          875\n",
      "Biden         510\n",
      "Harris        459\n",
      "Trump         411\n",
      "Beirut        288\n",
      "Name: count, dtype: int64\n",
      "Processing topic: FEMA\n",
      "Top entities for FEMA:\n",
      " Trump         1237\n",
      "Harris        1180\n",
      "Georgia        357\n",
      "Biden          355\n",
      "California     294\n",
      "Combs          293\n",
      "Texas          282\n",
      "Israel         274\n",
      "America        253\n",
      "Florida        234\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Abortion\n",
      "Top entities for Abortion:\n",
      " Trump       2572\n",
      "Harris      1889\n",
      "Vance        610\n",
      "Biden        537\n",
      "Walz         387\n",
      "GOP          315\n",
      "Georgia      295\n",
      "Senate       258\n",
      "Robinson     215\n",
      "Florida      210\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Inflation\n",
      "Top entities for Inflation:\n",
      " Trump       1648\n",
      "Harris      1316\n",
      "Fed          965\n",
      "Biden        553\n",
      "China        291\n",
      "Forbes       279\n",
      "America      196\n",
      "Israel       190\n",
      "Vance        181\n",
      "Michigan     175\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Unemployment\n",
      "Top entities for Unemployment:\n",
      " Fed       473\n",
      "Trump     221\n",
      "Harris    182\n",
      "China     111\n",
      "FOMC       98\n",
      "Biden      95\n",
      "Forbes     88\n",
      "Israel     60\n",
      "Gaza       51\n",
      "Powell     41\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Dockworkers\n",
      "Top entities for Dockworkers:\n",
      " Trump        3669\n",
      "Harris       1753\n",
      "Israel       1122\n",
      "Biden         876\n",
      "Florida       758\n",
      "Vance         586\n",
      "GOP           549\n",
      "Hezbollah     474\n",
      "Lebanon       453\n",
      "NFL           426\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Immigration\n",
      "Top entities for Immigration:\n",
      " Trump          3669\n",
      "Harris         2295\n",
      "Biden           963\n",
      "Vance           778\n",
      "Springfield     672\n",
      "Ohio            595\n",
      "GOP             367\n",
      "Walz            356\n",
      "America         313\n",
      "Congress        286\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Timmy\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Function to extract named entities from a document\n",
    "def extract_entities_from_docs(docs):\n",
    "    entity_lists = []\n",
    "    for doc in docs:\n",
    "        entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"LOC\"] and ent.text.isalpha() and len(ent.text) > 2]\n",
    "        entity_lists.append(entities)\n",
    "    return entity_lists\n",
    "\n",
    "# Loop through all DataFrames\n",
    "for topic, df in cleaned_dataframes.items():\n",
    "    print(f\"Processing topic: {topic}\")\n",
    "    \n",
    "    # Filter out rows with missing data in 'title' and 'content'\n",
    "    df = df.dropna(subset=['title', 'content'])\n",
    "\n",
    "    # Batch processing using nlp.pipe\n",
    "    title_docs = nlp.pipe(df['title'], batch_size=50, disable=[\"textcat\"])\n",
    "    content_docs = nlp.pipe(df['content'], batch_size=50, disable=[\"textcat\"])\n",
    "    \n",
    "    df['Title_Entities'] = extract_entities_from_docs(title_docs)\n",
    "    df['Content_Entities'] = extract_entities_from_docs(content_docs)\n",
    "\n",
    "    # Combine all entity lists into one for counting\n",
    "    all_entities = df['Title_Entities'].sum() + df['Content_Entities'].sum()\n",
    "    \n",
    "    # Calculate the value counts\n",
    "    entity_counts = pd.Series(all_entities).value_counts()\n",
    "    \n",
    "    # Output the top 10 most frequent entities\n",
    "    print(f\"Top entities for {topic}:\\n\", entity_counts.head(10))\n",
    "\n",
    "    # Store the counts for further analysis\n",
    "    cleaned_dataframes[topic]['Entity_Counts'] = entity_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
