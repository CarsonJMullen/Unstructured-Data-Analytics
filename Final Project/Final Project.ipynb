{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b1dc68",
   "metadata": {},
   "source": [
    "**Members:** Ethan Wong, Timmy Ren, Mason Shu, Medha Nalamada, Carson Mullen, Bethel Kim\n",
    "\n",
    "**Morning Cohort**: 11 AM - 1 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7563a96",
   "metadata": {},
   "source": [
    "**Current Tasks Remaining:**\n",
    "\n",
    "1. Verify Relevant Actors Code --> Timmy\n",
    "2. Perform LDA --> Mason\n",
    "3. Perform Active/Passive/Topic Voice Analysis --> Timmy\n",
    "4. Use ChatGPT to generate sentiment for actors in a given category (-100 to 100) --> Ethan\n",
    "5. Create a logical evaluation score metric based on active/passive voice, sentiment, title, and number of articles about certain subjects etc.(Come up with a few options) --> Bethel and Medha (Timmy and Mason will talk to Barua)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb917e6",
   "metadata": {},
   "source": [
    "# Install and Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4f7e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install eventregistry\n",
    "!python -m spacy download en_core_web_lg\n",
    "from eventregistry import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import spacy # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928fbe1",
   "metadata": {},
   "source": [
    "# Scraping Articles with Event Registry API - Ethan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the API key from the JSON file\n",
    "with open(\"config.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "api_key = config[\"api_key\"]\n",
    "\n",
    "# Initialize EventRegistry with the API key\n",
    "er = EventRegistry(apiKey=api_key, allowUseOfArchive=False)\n",
    "\n",
    "# Define topics to search for\n",
    "topics = [\n",
    "    \"Donald Trump\", \"Kamala Harris\", \"Israel\", \"Palestine\", \"Palestinians\", \"Hamas\", \"FEMA\", \"Abortion\",\n",
    "    \"Inflation\", \"Unemployment\", \"Economy\", \"Dockworkers\", \"ILA Port\", \"Immigration\"\n",
    "]\n",
    "\n",
    "# Define sources to search for and get their URIs\n",
    "source_names = [\"NPR\", \"MSNBC\", \"AP News\", \"FOX\", \"Forbes\"]\n",
    "source_uris = {source: er.getNewsSourceUri(source) for source in source_names}\n",
    "\n",
    "# List to store the names of all generated DataFrames\n",
    "dataframe_names = []\n",
    "\n",
    "# Loop through each topic\n",
    "for topic in topics:\n",
    "    # Get the URI for the concept\n",
    "    concept_uri = er.getConceptUri(topic)\n",
    "    \n",
    "    # List to hold all articles' data for the current topic (across all sources)\n",
    "    articles_data = []\n",
    "    \n",
    "    # Loop through each source individually\n",
    "    for source_name, source_uri in source_uris.items():\n",
    "        # Define the query for each topic and source\n",
    "        q = QueryArticlesIter(\n",
    "            conceptUri=concept_uri,\n",
    "            sourceUri=source_uri,\n",
    "            sourceLocationUri=er.getLocationUri(\"United States\"),  # Only US sources\n",
    "        )\n",
    "\n",
    "        # Fetch and accumulate up to 500 articles for the current topic from this source\n",
    "        for art in q.execQuery(er, sortBy=\"date\", maxItems=500):\n",
    "            articles_data.append({\n",
    "                \"title\": art.get(\"title\"),\n",
    "                \"source\": art.get(\"source\", {}).get(\"title\"),\n",
    "                \"author\": art.get(\"author\"),\n",
    "                \"url\": art.get(\"url\"),\n",
    "                \"publishedAt\": art.get(\"dateTime\"),\n",
    "                \"content\": art.get(\"body\")\n",
    "            })\n",
    "\n",
    "    # Create a single DataFrame for the current topic with articles from all sources\n",
    "    articles_df = pd.DataFrame(articles_data)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    file_name = f\"{topic.replace(' ', '_')}_articles.csv\"\n",
    "    articles_df.to_csv(file_name, index=False)\n",
    "\n",
    "    # Dynamically set the DataFrame name based on the topic, replacing spaces with underscores\n",
    "    df_name = f\"{topic.replace(' ', '_')}_df\"\n",
    "    globals()[df_name] = articles_df\n",
    "\n",
    "    # Append the DataFrame name to the list\n",
    "    dataframe_names.append(df_name)\n",
    "\n",
    "# Print the list of all generated DataFrame names\n",
    "print(\"Generated DataFrames:\", dataframe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Related Topics into Larger Dataframes and Cleaning Dataframes - Mason and Carson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67e70657-40d7-4f1a-be52-f5346f4dfe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine related dataframes\n",
    "\n",
    "# Read in csv's\n",
    "# List of topics\n",
    "topics = [\n",
    "    \"Donald Trump\", \"Kamala Harris\", \"Israel\", \"Palestine\", \"Palestinians\", \"Hamas\", \"FEMA\", \"Abortion\",\n",
    "    \"Inflation\", \"Unemployment\", \"Economy\", \"Dockworkers\", \"ILA Port\", \"Immigration\"\n",
    "]\n",
    "\n",
    "# Dictionary to hold the DataFrames after reading them from CSV\n",
    "dataframes = {}\n",
    "\n",
    "# Loop to read each CSV and store the DataFrame in the dictionary\n",
    "for topic in topics:\n",
    "    # Replace spaces with underscores to match your file naming convention\n",
    "    file_name = f\"{topic.replace(' ', '_')}_articles.csv\"\n",
    "    try:\n",
    "        dataframes[topic.replace(' ', '_')] = pd.read_csv(file_name)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")  # If a file is not found\n",
    "\n",
    "# Merge Donald Trump and Kamala Harris\n",
    "Donald_Trump_Kamala_Harris_df = pd.concat([dataframes[\"Donald_Trump\"], dataframes[\"Kamala_Harris\"]])\n",
    "\n",
    "# Merge Israel, Palestine, Palestinians, and Hamas\n",
    "Israel_Palestine_Palestinians_Hamas_df = pd.concat([\n",
    "    dataframes[\"Israel\"], dataframes[\"Palestine\"], dataframes[\"Palestinians\"], dataframes[\"Hamas\"]\n",
    "])\n",
    "\n",
    "# Merge Inflation, Unemployment, and Economy\n",
    "Inflation_Unemployment_Economy_df = pd.concat([\n",
    "    dataframes[\"Inflation\"], dataframes[\"Unemployment\"], dataframes[\"Economy\"]\n",
    "])\n",
    "\n",
    "# Merge Dockworkers and ILA Port\n",
    "Dockworkers_ILA_Port_df = pd.concat([\n",
    "    dataframes[\"Dockworkers\"], dataframes[\"ILA_Port\"]\n",
    "])\n",
    "\n",
    "# Add the merged dfs to dataframes\n",
    "dataframes[\"Donald_Trump_Kamala_Harris\"] = Donald_Trump_Kamala_Harris_df\n",
    "dataframes[\"Israel_Palestine_Palestinians_Hamas\"] = Israel_Palestine_Palestinians_Hamas_df\n",
    "dataframes[\"Inflation_Unemployment_Economy\"] = Inflation_Unemployment_Economy_df\n",
    "dataframes[\"Dockworkers_ILA_Port\"] = Dockworkers_ILA_Port_df\n",
    "\n",
    "# Remove topics that became merged\n",
    "del_keys = [\"Donald_Trump\", \"Kamala_Harris\", \"Israel\", \"Palestine\", \"Palestinians\", \"Hamas\",\n",
    "            \"Inflation\", \"Unemployment\", \"Economy\", \"Dockworkers\", \"ILA_Port\"]\n",
    "for key in del_keys:\n",
    "    del dataframes[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46875a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframes\n",
    "def count_sentences(text):\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    return len([s for s in sentences if s.strip()])\n",
    "\n",
    "def clean_df(df):\n",
    "    df = df.drop_duplicates().copy()\n",
    "    df.loc[:, 'content'] = df['content'].str.replace(\n",
    "        \"By entering your email and pushing continue, you are agreeing to Fox News\\' Terms of Use and Privacy Policy, which includes our Notice of Financial Incentive.\\n\\n\", \n",
    "        \"\", \n",
    "        regex=False\n",
    "    )\n",
    "    df.loc[:, 'num_sentences'] = df['content'].apply(count_sentences)\n",
    "    return df\n",
    "\n",
    "# Apply clean_df to each DataFrame in the dictionary\n",
    "cleaned_dataframes = {key: clean_df(df) for key, df in dataframes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3675259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: FEMA\n",
      "Top nouns for FEMA:\n",
      " people    2470\n",
      "women     2359\n",
      "woman     2216\n",
      "year      2035\n",
      "time      1933\n",
      "Trump     1880\n",
      "years     1591\n",
      "Harris    1590\n",
      "life      1225\n",
      "state     1135\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Abortion\n",
      "Top nouns for Abortion:\n",
      " Trump        3574\n",
      "Harris       2608\n",
      "abortion     2331\n",
      "state        1280\n",
      "debate       1141\n",
      "voters       1095\n",
      "President     965\n",
      "people        905\n",
      "Vance         824\n",
      "women         810\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Immigration\n",
      "Top nouns for Immigration:\n",
      " Trump          5218\n",
      "Harris         3717\n",
      "people         2147\n",
      "border         1598\n",
      "President      1576\n",
      "Biden          1491\n",
      "immigration    1360\n",
      "debate         1265\n",
      "year           1257\n",
      "voters         1176\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Donald_Trump_Kamala_Harris\n",
      "Top nouns for Donald_Trump_Kamala_Harris:\n",
      " Trump        16034\n",
      "Harris        9368\n",
      "President     4762\n",
      "election      3946\n",
      "Biden         3915\n",
      "people        3854\n",
      "campaign      3254\n",
      "president     3227\n",
      "state         2965\n",
      "year          2771\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Israel_Palestine_Palestinians_Hamas\n",
      "Top nouns for Israel_Palestine_Palestinians_Hamas:\n",
      " Israel       5836\n",
      "Hezbollah    2691\n",
      "Gaza         1952\n",
      "war          1892\n",
      "people       1827\n",
      "Lebanon      1810\n",
      "Trump        1791\n",
      "Hamas        1740\n",
      "Harris       1716\n",
      "Iran         1663\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Inflation_Unemployment_Economy\n",
      "Top nouns for Inflation_Unemployment_Economy:\n",
      " Trump      4711\n",
      "Harris     4055\n",
      "year       3680\n",
      "economy    2650\n",
      "people     2648\n",
      "years      2398\n",
      "rate       2391\n",
      "time       2187\n",
      "market     2107\n",
      "rates      1980\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Dockworkers_ILA_Port\n",
      "Top nouns for Dockworkers_ILA_Port:\n",
      " Trump        5143\n",
      "people       3339\n",
      "Harris       2657\n",
      "time         2642\n",
      "year         2530\n",
      "years        1778\n",
      "state        1776\n",
      "week         1546\n",
      "election     1431\n",
      "President    1379\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Finding relevant actors with noun extraction\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Function to extract nouns and proper nouns\n",
    "def extract_nouns(text):\n",
    "    if pd.isnull(text):  # Handle missing values\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    nouns = [token.text for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"] and token.is_alpha and len(token.text) > 2]\n",
    "    return nouns\n",
    "\n",
    "# Loop through all DataFrames\n",
    "for topic, df in cleaned_dataframes.items():\n",
    "    print(f\"Processing topic: {topic}\")\n",
    "    \n",
    "    # Apply the noun extraction to both the 'title' and 'content' columns\n",
    "    df['Title_Nouns'] = df['title'].apply(extract_nouns)\n",
    "    df['Content_Nouns'] = df['content'].apply(extract_nouns)\n",
    "\n",
    "    # Combine all noun lists into one for counting\n",
    "    all_nouns = df['Title_Nouns'].sum() + df['Content_Nouns'].sum()  # Flatten lists so that the noun lists from both the Title_Nouns and Content_Nouns columns are in a single list\n",
    "    \n",
    "    # Calculate the value counts\n",
    "    noun_counts = pd.Series(all_nouns).value_counts()\n",
    "    \n",
    "    # Output the top 10 most frequent nouns\n",
    "    print(f\"Top nouns for {topic}:\\n\", noun_counts.head(10))\n",
    "\n",
    "    # Store the counts for further analysis\n",
    "    cleaned_dataframes[topic]['Noun_Counts'] = noun_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
