{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b1dc68",
   "metadata": {},
   "source": [
    "**Members:** Ethan Wong, Timmy Ren, Mason Shu, Medha Nalamada, Carson Mullen, Bethel Kim\n",
    "\n",
    "**Morning Cohort**: 11 AM - 1 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7563a96",
   "metadata": {},
   "source": [
    "**Current Tasks Remaining:**\n",
    "\n",
    "1. Perform Active/Passive/Topic Voice Analysis --> Timmy\n",
    "2. Use ChatGPT to generate sentiment for actors in a given category (-100 to 100) --> Ethan\n",
    "3. Implement Bias Score Metric --> Timmy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb917e6",
   "metadata": {},
   "source": [
    "# Install and Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f7e67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 2.4/12.8 MB 12.2 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 13.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 22.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 19.6 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#!pip install eventregistry\n",
    "!python -m spacy download en_core_web_lg\n",
    "#!python -m spacy download en_core_web_sm\n",
    "from eventregistry import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import spacy # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928fbe1",
   "metadata": {},
   "source": [
    "# Scraping Articles with Event Registry API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcaf719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated DataFrames: ['Election_df', 'Gaza_df', 'FEMA_df', 'Abortion_df', 'Inflation_df', 'Unemployment_df', 'Dockworkers_df', 'Immigration_df']\n"
     ]
    }
   ],
   "source": [
    "# Load the API key from the JSON file\n",
    "with open(\"config.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "api_key = config[\"api_key\"]\n",
    "\n",
    "# Initialize EventRegistry with the API key\n",
    "er = EventRegistry(apiKey=api_key, allowUseOfArchive=False)\n",
    "\n",
    "# Define topics to search for\n",
    "#topics = [\n",
    "    #\"Donald Trump\", \"Kamala Harris\", \"Israel\", \"Palestine\", \"Palestinians\", \"Hamas\", \"FEMA\", \"Abortion\",\n",
    "    #\"Inflation\", \"Unemployment\", \"Economy\", \"Dockworkers\", \"ILA Port\", \"Immigration\"\n",
    "#]\n",
    "\n",
    "topics = [\"Election\", \"Gaza\", \"FEMA\", \"Abortion\", \"Inflation\", \"Unemployment\", \"Dockworkers\", \"Immigration\"]\n",
    "\n",
    "# Define sources to search for and get their URIs\n",
    "source_names = [\"NPR\", \"MSNBC\", \"AP News\", \"FOX\", \"Forbes\"]\n",
    "source_uris = {source: er.getNewsSourceUri(source) for source in source_names}\n",
    "\n",
    "# List to store the names of all generated DataFrames\n",
    "dataframe_names = []\n",
    "\n",
    "# Loop through each topic\n",
    "for topic in topics:\n",
    "    # Get the URI for the concept\n",
    "    concept_uri = er.getConceptUri(topic)\n",
    "    \n",
    "    # List to hold all articles' data for the current topic (across all sources)\n",
    "    articles_data = []\n",
    "    \n",
    "    # Loop through each source individually\n",
    "    for source_name, source_uri in source_uris.items():\n",
    "        # Define the query for each topic and source\n",
    "        q = QueryArticlesIter(\n",
    "            conceptUri=concept_uri,\n",
    "            sourceUri=source_uri,\n",
    "            sourceLocationUri=er.getLocationUri(\"United States\"),  # Only US sources\n",
    "        )\n",
    "\n",
    "        # Fetch and accumulate up to 500 articles for the current topic from this source\n",
    "        for art in q.execQuery(er, sortBy=\"date\", maxItems=500):\n",
    "            articles_data.append({\n",
    "                \"title\": art.get(\"title\"),\n",
    "                \"source\": art.get(\"source\", {}).get(\"title\"),\n",
    "                \"author\": art.get(\"author\"),\n",
    "                \"url\": art.get(\"url\"),\n",
    "                \"publishedAt\": art.get(\"dateTime\"),\n",
    "                \"content\": art.get(\"body\")\n",
    "            })\n",
    "\n",
    "    # Create a single DataFrame for the current topic with articles from all sources\n",
    "    articles_df = pd.DataFrame(articles_data)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    file_name = f\"{topic.replace(' ', '_')}_articles.csv\"\n",
    "    articles_df.to_csv(file_name, index=False)\n",
    "\n",
    "    # Dynamically set the DataFrame name based on the topic, replacing spaces with underscores\n",
    "    df_name = f\"{topic.replace(' ', '_')}_df\"\n",
    "    globals()[df_name] = articles_df\n",
    "\n",
    "    # Append the DataFrame name to the list\n",
    "    dataframe_names.append(df_name)\n",
    "\n",
    "# Print the list of all generated DataFrame names\n",
    "print(\"Generated DataFrames:\", dataframe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1055b4b",
   "metadata": {},
   "source": [
    "# Cleaning Dataframes and Running Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f3f3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in CSVs dynamically\n",
    "\n",
    "topics = [\"Election\", \"Gaza\", \"FEMA\", \"Abortion\", \"Inflation\", \"Unemployment\", \"Dockworkers\", \"Immigration\"]\n",
    "\n",
    "# Dictionary to hold the DataFrames after reading them from CSV\n",
    "dataframes = {}\n",
    "\n",
    "# Loop to read each CSV and store the DataFrame in the dictionary\n",
    "for topic in topics:\n",
    "    # Replace spaces with underscores to match your file naming convention\n",
    "    file_name = f\"{topic.replace(' ', '_')}_articles.csv\"\n",
    "    try:\n",
    "        dataframes[topic.replace(' ', '_')] = pd.read_csv(file_name)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")  # If a file is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46875a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframes\n",
    "def count_sentences(text):\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    return len([s for s in sentences if s.strip()])\n",
    "\n",
    "def clean_df(df):\n",
    "    df = df.drop_duplicates().copy()\n",
    "    df.loc[:, 'content'] = df['content'].str.replace(\n",
    "        \"By entering your email and pushing continue, you are agreeing to Fox News\\' Terms of Use and Privacy Policy, which includes our Notice of Financial Incentive.\\n\\n\", \n",
    "        \"\", \n",
    "        regex=False\n",
    "    )\n",
    "    df.loc[:, 'num_sentences'] = df['content'].apply(count_sentences)\n",
    "    return df\n",
    "\n",
    "# Apply clean_df to each DataFrame in the dictionary\n",
    "cleaned_dataframes = {key: clean_df(df) for key, df in dataframes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2127e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: Election\n",
      "Top entities for Election:\n",
      " Trump           7078\n",
      "Harris          4119\n",
      "Biden           1937\n",
      "Vance            963\n",
      "GOP              826\n",
      "Georgia          787\n",
      "Congress         659\n",
      "Walz             654\n",
      "Pennsylvania     638\n",
      "Israel           618\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Gaza\n",
      "Top entities for Gaza:\n",
      " Israel       3739\n",
      "Hezbollah    1708\n",
      "Gaza         1339\n",
      "Lebanon      1300\n",
      "Hamas        1143\n",
      "Iran          875\n",
      "Biden         510\n",
      "Harris        459\n",
      "Trump         411\n",
      "Beirut        288\n",
      "Name: count, dtype: int64\n",
      "Processing topic: FEMA\n",
      "Top entities for FEMA:\n",
      " Trump         1237\n",
      "Harris        1180\n",
      "Georgia        357\n",
      "Biden          355\n",
      "California     294\n",
      "Combs          293\n",
      "Texas          282\n",
      "Israel         274\n",
      "America        253\n",
      "Florida        234\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Abortion\n",
      "Top entities for Abortion:\n",
      " Trump       2572\n",
      "Harris      1889\n",
      "Vance        610\n",
      "Biden        537\n",
      "Walz         387\n",
      "GOP          315\n",
      "Georgia      295\n",
      "Senate       258\n",
      "Robinson     215\n",
      "Florida      210\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Inflation\n",
      "Top entities for Inflation:\n",
      " Trump       1648\n",
      "Harris      1316\n",
      "Fed          965\n",
      "Biden        553\n",
      "China        291\n",
      "Forbes       279\n",
      "America      196\n",
      "Israel       190\n",
      "Vance        181\n",
      "Michigan     175\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Unemployment\n",
      "Top entities for Unemployment:\n",
      " Fed       473\n",
      "Trump     221\n",
      "Harris    182\n",
      "China     111\n",
      "FOMC       98\n",
      "Biden      95\n",
      "Forbes     88\n",
      "Israel     60\n",
      "Gaza       51\n",
      "Powell     41\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Dockworkers\n",
      "Top entities for Dockworkers:\n",
      " Trump        3669\n",
      "Harris       1753\n",
      "Israel       1122\n",
      "Biden         876\n",
      "Florida       758\n",
      "Vance         586\n",
      "GOP           549\n",
      "Hezbollah     474\n",
      "Lebanon       453\n",
      "NFL           426\n",
      "Name: count, dtype: int64\n",
      "Processing topic: Immigration\n",
      "Top entities for Immigration:\n",
      " Trump          3669\n",
      "Harris         2295\n",
      "Biden           963\n",
      "Vance           778\n",
      "Springfield     672\n",
      "Ohio            595\n",
      "GOP             367\n",
      "Walz            356\n",
      "America         313\n",
      "Congress        286\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Running name entity recognition on the data\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Function to extract named entities from a document\n",
    "def extract_entities_from_docs(docs):\n",
    "    entity_lists = []\n",
    "    for doc in docs:\n",
    "        entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"LOC\"] and ent.text.isalpha() and len(ent.text) > 2]\n",
    "        entity_lists.append(entities)\n",
    "    return entity_lists\n",
    "\n",
    "# Loop through all DataFrames\n",
    "for topic, df in cleaned_dataframes.items():\n",
    "    print(f\"Processing topic: {topic}\")\n",
    "    \n",
    "    # Filter out rows with missing data in 'title' and 'content'\n",
    "    df = df.dropna(subset=['title', 'content'])\n",
    "\n",
    "    # nlp.pipe is btch processing\n",
    "    title_docs = nlp.pipe(df['title'], disable=[\"textcat\"])\n",
    "    content_docs = nlp.pipe(df['content'], disable=[\"textcat\"])\n",
    "    \n",
    "    df['Title_Entities'] = extract_entities_from_docs(title_docs)\n",
    "    df['Content_Entities'] = extract_entities_from_docs(content_docs)\n",
    "\n",
    "    # Combine all entity lists into one for counting\n",
    "    all_entities = df['Title_Entities'].sum() + df['Content_Entities'].sum()\n",
    "    \n",
    "    # Calculate the value counts\n",
    "    entity_counts = pd.Series(all_entities).value_counts()\n",
    "    \n",
    "    # Output the top 10 most frequent entities\n",
    "    print(f\"Top entities for {topic}:\\n\", entity_counts.head(10))\n",
    "\n",
    "    # Store the counts for further analysis\n",
    "    cleaned_dataframes[topic]['Entity_Counts'] = entity_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031dbda8",
   "metadata": {},
   "source": [
    "Based on the data, we determined that these were the relevant actors for each topic:\n",
    "\n",
    "* **Election:** Trump, Harris, Biden\n",
    "* **Gaza:** Israel, Hamas, Palestinians\n",
    "* **FEMA:** Trump, Harris\n",
    "* **Abortion**: Trump, Harris, Women\n",
    "* **Unemployment**: Fed, Trump, Harris \n",
    "* **Dockworkers**: Dockworkers\n",
    "* **Immigration**: Trump, Harris, Biden, Haitian\n",
    "\n",
    "In order to process the articles more efficiently for subsequent steps, we decided to remove any sentences that do not contain the above words. Additionally, we will do the following for specific topics:\n",
    "\n",
    "* **Gaza:** Find and Replace Civilian\n",
    "* **Abortion**: Check for instances of \"Woman\" as well\n",
    "* **Dockworkers**: Find and replace Union, International Longshoremen's Association; Find and replace United States Maritime Alliance, USMX\n",
    "* **Immigration**: Find and replace Springfield\n",
    "\n",
    "Our reasoning is that now, the articles will be smaller in size while still retaining all of the articles, and that passive/active voice is not getting affected by non-actor sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c861661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and replace for each of the sentences\n",
    "# For article in a given topic, remove any sentences that do not contain these words. \n",
    "# Idea is that articles are smaller and easier to process while still retaining the articles \n",
    "# Passive/Active voice is not getting affected by non-actor sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d223621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keywords and find-and-replace mappings for each topic\n",
    "topic_keywords = {\n",
    "    \"Election\": {\n",
    "        \"keywords\": [\"Trump\", \"Harris\", \"Biden\"],\n",
    "    },\n",
    "    \"Gaza\": {\n",
    "        \"keywords\": [\"Israel\", \"Hamas\", \"Palestinians\"],\n",
    "        \"find_replace\": {\"Palestinians\": \"civilian\"}\n",
    "    },\n",
    "    \"FEMA\": {\n",
    "        \"keywords\": [\"Trump\", \"Harris\"],\n",
    "    },\n",
    "    \"Abortion\": {\n",
    "        \"keywords\": [\"Trump\", \"Harris\", \"Women\", \"Woman\"],\n",
    "    },\n",
    "    \"Unemployment\": {\n",
    "        \"keywords\": [\"Fed\", \"Trump\", \"Harris\"],\n",
    "    },\n",
    "    \"Dockworkers\": {\n",
    "        \"keywords\": [\"Dockworkers\", \"Employer\"],\n",
    "        \"find_replace\": {\n",
    "            \"union\": \"Dockworkers\",\n",
    "            \"international longshoremen's association\": \"Dockworkers\",\n",
    "            \"United States Maritime Alliance\": \"Employer\",\n",
    "            \"USMX\": \"Employer\"\n",
    "        }\n",
    "    },\n",
    "    \"Immigration\": {\n",
    "        \"keywords\": [\"Trump\", \"Harris\", \"Biden\", \"Haitian\"],\n",
    "        \"find_replace\": {\"Springfield\": \"Haitian\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to perform find-and-replace operations\n",
    "def apply_find_replace(text, find_replace_dict):\n",
    "    # Build a mapping from lowercased keys to their replacements\n",
    "    lower_find_replace = {k.lower(): v for k, v in find_replace_dict.items()}\n",
    "    # Escape special characters in keys for regex\n",
    "    escaped_keys = [re.escape(k) for k in find_replace_dict.keys()]\n",
    "    pattern = re.compile(\"|\".join(escaped_keys), re.IGNORECASE)\n",
    "    \n",
    "    def replace_match(m):\n",
    "        matched_text = m.group(0)\n",
    "        # Lookup the replacement using the lowercase matched text\n",
    "        replacement = lower_find_replace.get(matched_text.lower(), matched_text)\n",
    "        return replacement\n",
    "    \n",
    "    return pattern.sub(replace_match, text)\n",
    "\n",
    "# Function to filter sentences based on keywords\n",
    "def filter_sentences(text, keywords):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    # Compile regex patterns for keywords (case-insensitive)\n",
    "    keyword_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, keywords)) + r')\\b', re.IGNORECASE)\n",
    "    filtered_sentences = [sent for sent in sentences if keyword_pattern.search(sent)]\n",
    "    return ' '.join(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08e10685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: Election\n",
      "Processing topic: Gaza\n",
      "Processing topic: FEMA\n",
      "Processing topic: Abortion\n",
      "Processing topic: Inflation\n",
      "Processing topic: Unemployment\n",
      "Processing topic: Dockworkers\n",
      "Processing topic: Immigration\n"
     ]
    }
   ],
   "source": [
    "# Apply the filtering to each DataFrame\n",
    "for topic, df in cleaned_dataframes.items():\n",
    "    print(f\"Processing topic: {topic}\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Get keywords and find-and-replace mappings for the topic\n",
    "    keywords = topic_keywords.get(topic, {}).get('keywords', [])\n",
    "    find_replace = topic_keywords.get(topic, {}).get('find_replace', {})\n",
    "    \n",
    "    # Apply find-and-replace and sentence filtering to 'content' column\n",
    "    filtered_contents = []\n",
    "    for content in df['content']:\n",
    "        # Skip if content is NaN\n",
    "        if pd.isnull(content):\n",
    "            filtered_contents.append(content)\n",
    "            continue\n",
    "        \n",
    "        # Apply find-and-replace operations\n",
    "        if find_replace:\n",
    "            content = apply_find_replace(content, find_replace)\n",
    "        \n",
    "        # Filter sentences based on keywords\n",
    "        filtered_content = filter_sentences(content, keywords)\n",
    "        filtered_contents.append(filtered_content)\n",
    "    \n",
    "    # Update the DataFrame with the filtered content\n",
    "    df['filtered_content'] = filtered_contents\n",
    "    cleaned_dataframes[topic] = df\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    #df.to_csv(f\"{topic.replace(' ', '_')}_filtered_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2df41f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: Election, Original Articles: 1967, After Filtering: 1502\n",
      "Topic: Gaza, Original Articles: 481, After Filtering: 442\n",
      "Topic: FEMA, Original Articles: 1429, After Filtering: 367\n",
      "Topic: Abortion, Original Articles: 539, After Filtering: 501\n",
      "Topic: Unemployment, Original Articles: 183, After Filtering: 107\n",
      "Topic: Dockworkers, Original Articles: 2500, After Filtering: 117\n",
      "Topic: Immigration, Original Articles: 937, After Filtering: 735\n"
     ]
    }
   ],
   "source": [
    "# Create a second version of cleaned_dataframes where articles with empty 'filtered_content' are removed\n",
    "cleaned_dataframes_filtered = {}\n",
    "for topic, df in cleaned_dataframes.items():\n",
    "    # Remove rows where 'filtered_content' is empty or contains only whitespace\n",
    "    df_filtered = df[df['filtered_content'].str.strip().astype(bool)].copy()\n",
    "    cleaned_dataframes_filtered[topic] = df_filtered\n",
    "\n",
    "    # Save the filtered DataFrame to a new CSV file (second version)\n",
    "    #df_filtered.to_csv(f\"{topic.replace(' ', '_')}_filtered_articles_no_empty.csv\", index=False)\n",
    "\n",
    "# Print the number of articles before and after filtering\n",
    "for topic in topic_keywords.keys():\n",
    "    original_count = len(cleaned_dataframes[topic])\n",
    "    filtered_count = len(cleaned_dataframes_filtered[topic])\n",
    "    print(f\"Topic: {topic}, Original Articles: {original_count}, After Filtering: {filtered_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d132d662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>filtered_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Wait Wait' for October 12, 2024: With Not My ...</td>\n",
       "      <td>This week's show was recorded in Chicago with ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What the Harris campaign is doing to try to wi...</td>\n",
       "      <td>ROCKY MOUNT, N.C. -- From her barber chair and...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hurricane Evacuation Saves Lives, Mass Gatheri...</td>\n",
       "      <td>Hurricane Evacuation Saves Lives, Mass Gatheri...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Some Democrats are still hesitant to vote for ...</td>\n",
       "      <td>&lt;iframe src=\"https://www.npr.org/player/embed/...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harris releases medical report, drawing anothe...</td>\n",
       "      <td>US Vice President and Democratic presidential ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  'Wait Wait' for October 12, 2024: With Not My ...   \n",
       "1  What the Harris campaign is doing to try to wi...   \n",
       "2  Hurricane Evacuation Saves Lives, Mass Gatheri...   \n",
       "3  Some Democrats are still hesitant to vote for ...   \n",
       "4  Harris releases medical report, drawing anothe...   \n",
       "\n",
       "                                             content filtered_content  \n",
       "0  This week's show was recorded in Chicago with ...                   \n",
       "1  ROCKY MOUNT, N.C. -- From her barber chair and...                   \n",
       "2  Hurricane Evacuation Saves Lives, Mass Gatheri...                   \n",
       "3  <iframe src=\"https://www.npr.org/player/embed/...                   \n",
       "4  US Vice President and Democratic presidential ...                   "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the filtered DataFrame for a topic\n",
    "topic = \"Dockworkers\"\n",
    "cleaned_dataframes[topic][['title', 'content', 'filtered_content']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
