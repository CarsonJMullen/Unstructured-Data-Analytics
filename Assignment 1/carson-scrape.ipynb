{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 435\n",
      "Scraping page 434\n",
      "Scraping page 433\n",
      "Scraping page 432\n",
      "Scraping page 431\n",
      "Scraping page 430\n",
      "Scraping page 429\n",
      "Scraping page 428\n",
      "Scraping page 427\n",
      "Scraping page 426\n",
      "Scraping page 425\n",
      "Scraping page 424\n",
      "Scraping page 423\n",
      "Scraping page 422\n",
      "Scraping page 421\n",
      "Scraping page 420\n",
      "Scraping page 419\n",
      "Scraping page 418\n",
      "Scraping page 417\n",
      "Scraping page 416\n",
      "Scraping page 415\n",
      "Scraping page 414\n",
      "Scraping page 413\n",
      "Scraping page 412\n",
      "Scraping page 411\n",
      "Scraping page 410\n",
      "Scraping page 409\n",
      "Scraping page 408\n",
      "Scraping page 407\n",
      "Scraping page 406\n",
      "Scraping page 405\n",
      "Scraping page 404\n",
      "Scraping page 403\n",
      "Scraping page 402\n",
      "Scraping page 401\n",
      "Scraping page 400\n",
      "Scraping page 399\n",
      "Scraping page 398\n",
      "Scraping page 397\n",
      "Scraping page 396\n",
      "Scraping page 395\n",
      "Scraping page 394\n",
      "Scraping page 393\n",
      "Scraping page 392\n",
      "Scraping page 391\n",
      "Scraping page 390\n",
      "Scraping page 389\n",
      "Scraping page 388\n",
      "Scraping page 387\n",
      "Scraping page 386\n",
      "Scraping page 385\n",
      "Scraping page 384\n",
      "Scraping page 383\n",
      "Scraping page 382\n",
      "Scraping page 381\n",
      "Scraping page 380\n",
      "Scraping page 379\n",
      "Scraping page 378\n",
      "Scraping page 377\n",
      "Scraping page 376\n",
      "Scraping page 375\n",
      "Scraping page 374\n",
      "Scraping page 373\n",
      "Scraping page 372\n",
      "Scraping page 371\n",
      "Scraping page 370\n",
      "Scraping page 369\n",
      "Scraping page 368\n",
      "Scraping page 367\n",
      "Scraping page 366\n",
      "Scraping page 365\n",
      "Scraping page 364\n",
      "Scraping page 363\n",
      "Scraping page 362\n",
      "Scraping page 361\n",
      "Scraping page 360\n",
      "Scraping page 359\n",
      "Scraping page 358\n",
      "Scraping page 357\n",
      "Scraping page 356\n",
      "Scraping page 355\n",
      "Scraping page 354\n",
      "Scraping page 353\n",
      "Scraping page 352\n",
      "Scraping page 351\n",
      "Scraping page 350\n",
      "Scraping page 349\n",
      "Scraping page 348\n",
      "Scraping page 347\n",
      "Scraping page 346\n",
      "Scraping page 345\n",
      "Scraping page 344\n",
      "Scraping page 343\n",
      "Scraping page 342\n",
      "Scraping page 341\n",
      "Scraping page 340\n",
      "Scraping page 339\n",
      "Scraping page 338\n",
      "Scraping page 337\n",
      "Scraping page 336\n",
      "Scraping page 335\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "last_page = 435\n",
    "text = []\n",
    "dates = []\n",
    "\n",
    "for i in range(last_page, 0, -1):\n",
    "    # Step 1: Fetch the HTML content from the website\n",
    "    url = f'https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p{i}'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:  # Check if the request was successful\n",
    "        html_content = response.text\n",
    "\n",
    "        # Step 2: Parse the HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Step 3: Remove all blockquote elements\n",
    "        for blockquote in soup.find_all('blockquote'):\n",
    "            blockquote.extract()\n",
    "\n",
    "        # Step 4: Grab elements that have the class \"Message\"\n",
    "        message_content = soup.find_all('div', class_='Message')\n",
    "\n",
    "        # Step 5: Grab all <time> elements\n",
    "        time_elements = soup.find_all('time')\n",
    "\n",
    "        # Step 6: Create lists of text\n",
    "        text.extend([message.get_text().strip() for message in message_content])\n",
    "        dates.extend([time.get_text().strip() for time in time_elements])\n",
    "\n",
    "        print(\"Scraping page\", i)\n",
    "\n",
    "        if len(text) >= 5000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'Message': text, 'Date': dates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And, that's on top of any discount you negotia...</td>\n",
       "      <td>February 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Add $350 to any lease on a Kia if you are not ...</td>\n",
       "      <td>February 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did you hear about the Key FOB scam the dealer...</td>\n",
       "      <td>February 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>That's a jaw-dropping lease incentive. Amazing...</td>\n",
       "      <td>February 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm pretty sure that's the case with any capti...</td>\n",
       "      <td>February 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>In TX I've gone 2 years without a front plate ...</td>\n",
       "      <td>March 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029</th>\n",
       "      <td>Got to love AZ, we do not need front plates......</td>\n",
       "      <td>March 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5030</th>\n",
       "      <td>Yes, since you are on the street and the cars ...</td>\n",
       "      <td>March 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>I see lots of single plates around here. I'm i...</td>\n",
       "      <td>March 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5032</th>\n",
       "      <td>They cut no slack in NH. I had a plate fall of...</td>\n",
       "      <td>March 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5033 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Message           Date\n",
       "0     And, that's on top of any discount you negotia...  February 2020\n",
       "1     Add $350 to any lease on a Kia if you are not ...  February 2020\n",
       "2     Did you hear about the Key FOB scam the dealer...  February 2020\n",
       "3     That's a jaw-dropping lease incentive. Amazing...  February 2020\n",
       "4     I'm pretty sure that's the case with any capti...  February 2020\n",
       "...                                                 ...            ...\n",
       "5028  In TX I've gone 2 years without a front plate ...     March 2014\n",
       "5029  Got to love AZ, we do not need front plates......     March 2014\n",
       "5030  Yes, since you are on the street and the cars ...     March 2014\n",
       "5031  I see lots of single plates around here. I'm i...     March 2014\n",
       "5032  They cut no slack in NH. I had a plate fall of...     March 2014\n",
       "\n",
       "[5033 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
