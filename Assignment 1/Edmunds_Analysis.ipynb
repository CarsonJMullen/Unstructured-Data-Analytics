{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292a437b-c143-4fa4-8901-e94c8c4d9c7a",
   "metadata": {},
   "source": [
    "**Team Members: Ethan Wong, Timmy Ren, Mason Shu, Medha Nalamada, Carson Mullen, Bethel Kim**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35e596-29fe-4a7a-b65a-dec20e12ec5c",
   "metadata": {},
   "source": [
    "*Note to all: Please pull any changes from the repo before working on this file!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2a6a0-7b21-4ec8-a4db-dd7413b3e0c7",
   "metadata": {},
   "source": [
    "# Scraping from Edmunds.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "69a8b31c-fcbf-4251-bc53-690acd790cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing necessary libraries\n",
    "\n",
    "# !pip install selenium\n",
    "# !pip install google-colab-selenium\n",
    "# !pip install nltk\n",
    "#!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "496b35aa-9693-4283-a43d-ae6c2ff1d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "142d7894-603e-44db-9913-a485e11293e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(sentences):\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        filtered_sentence = ' '.join(filtered_words)\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0e348-5d91-444b-9b53-013fba3922b5",
   "metadata": {},
   "source": [
    "We decided to choose the newest posts for our analyses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "869efe92-afee-4ddb-ae90-fe34f3260f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 434\n",
      "Scraping page 433\n",
      "Scraping page 432\n",
      "Scraping page 431\n",
      "Scraping page 430\n",
      "Scraping page 429\n",
      "Scraping page 428\n",
      "Scraping page 427\n",
      "Scraping page 426\n",
      "Scraping page 425\n",
      "Scraping page 424\n",
      "Scraping page 423\n",
      "Scraping page 422\n",
      "Scraping page 421\n",
      "Scraping page 420\n",
      "Scraping page 419\n",
      "Scraping page 418\n",
      "Scraping page 417\n",
      "Scraping page 416\n",
      "Scraping page 415\n",
      "Scraping page 414\n",
      "Scraping page 413\n",
      "Scraping page 412\n",
      "Scraping page 411\n",
      "Scraping page 410\n",
      "Scraping page 409\n",
      "Scraping page 408\n",
      "Scraping page 407\n",
      "Scraping page 406\n",
      "Scraping page 405\n",
      "Scraping page 404\n",
      "Scraping page 403\n",
      "Scraping page 402\n",
      "Scraping page 401\n",
      "Scraping page 400\n",
      "Scraping page 399\n",
      "Scraping page 398\n",
      "Scraping page 397\n",
      "Scraping page 396\n",
      "Scraping page 395\n",
      "Scraping page 394\n",
      "Scraping page 393\n",
      "Scraping page 392\n",
      "Scraping page 391\n",
      "Scraping page 390\n",
      "Scraping page 389\n",
      "Scraping page 388\n",
      "Scraping page 387\n",
      "Scraping page 386\n",
      "Scraping page 385\n",
      "Scraping page 384\n",
      "Scraping page 383\n",
      "Scraping page 382\n",
      "Scraping page 381\n",
      "Scraping page 380\n",
      "Scraping page 379\n",
      "Scraping page 378\n",
      "Scraping page 377\n",
      "Scraping page 376\n",
      "Scraping page 375\n",
      "Scraping page 374\n",
      "Scraping page 373\n",
      "Scraping page 372\n",
      "Scraping page 371\n",
      "Scraping page 370\n",
      "Scraping page 369\n",
      "Scraping page 368\n",
      "Scraping page 367\n",
      "Scraping page 366\n",
      "Scraping page 365\n",
      "Scraping page 364\n",
      "Scraping page 363\n",
      "Scraping page 362\n",
      "Scraping page 361\n",
      "Scraping page 360\n",
      "Scraping page 359\n",
      "Scraping page 358\n",
      "Scraping page 357\n",
      "Scraping page 356\n",
      "Scraping page 355\n",
      "Scraping page 354\n",
      "Scraping page 353\n",
      "Scraping page 352\n",
      "Scraping page 351\n",
      "Scraping page 350\n",
      "Scraping page 349\n",
      "Scraping page 348\n",
      "Scraping page 347\n",
      "Scraping page 346\n",
      "Scraping page 345\n",
      "Scraping page 344\n",
      "Scraping page 343\n",
      "Scraping page 342\n",
      "Scraping page 341\n",
      "Scraping page 340\n",
      "Scraping page 339\n",
      "Scraping page 338\n",
      "Scraping page 337\n",
      "Scraping page 336\n",
      "Collected 5000 unique posts, stopping at page 335\n"
     ]
    }
   ],
   "source": [
    "# Set up Selenium to use Chrome browser\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (without opening a browser window)\n",
    "chrome_service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
    "\n",
    "# Access the last page of the discussion\n",
    "last_page = 435 \n",
    "url = f'https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p{last_page}'\n",
    "driver.get(url)\n",
    "\n",
    "# Extract elements containing the messages and dates\n",
    "elements = driver.find_elements(\"xpath\", \"//div[contains(@class,'Message') and contains(@class,'userContent')]\")\n",
    "elements2 = driver.find_elements(\"xpath\", \"//span[@class='MItem DateCreated']//time\")\n",
    "\n",
    "text = []\n",
    "dates = []\n",
    "unique_messages = set()  # Set to track unique messages\n",
    "\n",
    "for element in elements:\n",
    "    # Find blockquotes within the element and remove their text\n",
    "    blockquote_elements = element.find_elements(\"xpath\", \".//blockquote\")\n",
    "    message = element.text\n",
    "\n",
    "    for blockquote in blockquote_elements:\n",
    "        message = message.replace(blockquote.text, \"\")\n",
    "\n",
    "    if message not in unique_messages:\n",
    "        text.append(message.strip())\n",
    "        unique_messages.add(message)\n",
    "\n",
    "for element in elements2:\n",
    "    dates.append(element.text)\n",
    "\n",
    "# Loop through pages in reverse order\n",
    "for i in range(last_page - 1, 0, -1):\n",
    "    url = f'https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p{i}'\n",
    "    driver.get(url)\n",
    "\n",
    "    elements = driver.find_elements(\"xpath\", \"//div[contains(@class,'Message') and contains(@class,'userContent')]\")\n",
    "    elements2 = driver.find_elements(\"xpath\", \"//span[@class='MItem DateCreated']//time\")\n",
    "\n",
    "    for element in elements:\n",
    "        # Find blockquotes within the element and remove their text\n",
    "        blockquote_elements = element.find_elements(\"xpath\", \".//blockquote\")\n",
    "        message = element.text\n",
    "        \n",
    "        for blockquote in blockquote_elements:\n",
    "            message = message.replace(blockquote.text, \"\")\n",
    "        \n",
    "        if message not in unique_messages:\n",
    "            text.append(message.strip())\n",
    "            unique_messages.add(message)\n",
    "        if len(text) >= 5000:\n",
    "            break  # Stop collecting once we have 5000 unique posts\n",
    "\n",
    "    for element in elements2:\n",
    "        dates.append(element.text)\n",
    "        if len(text) >= 5000:\n",
    "            break  # Stop collecting once we have 5000 unique posts\n",
    "\n",
    "    if len(text) >= 5000:\n",
    "        print(f\"Collected 5000 unique posts, stopping at page {i}\")\n",
    "        break\n",
    "\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Ensure the lengths of text and dates are the same before saving\n",
    "if len(text) > len(dates):\n",
    "    text = text[:len(dates)]\n",
    "elif len(dates) > len(text):\n",
    "    dates = dates[:len(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8a2769d3-c3be-43a7-a4ba-dc49d51ac740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame and save it to a CSV file\n",
    "df = pd.DataFrame({'Message': text, 'Date': dates})\n",
    "df.to_csv('ScrapedData.csv', index=False) # Use this for task A!\n",
    "\n",
    "# Remove stopwords and save the modified data\n",
    "text = remove_stopwords(text)\n",
    "df['Message'] = text\n",
    "df.to_csv('ScrapedDataMod.csv', index=False) # Use this for task B onwards!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90be00a-f2ef-432a-a8f4-1095a270f949",
   "metadata": {},
   "source": [
    "# Task A: Testing Zipf's law econometrically and plotting the most common 100 words in the data against the theoretical prediction of the law.\n",
    "\n",
    "Note: Stopwords not removed, and stemming or lemmatization not performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5086b42a-74d6-442e-ad66-56c7235d68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA = pd.read_csv('ScrapedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814fb02-4bbd-41bb-8aff-2ae7e2ac09e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2685e218-229b-4043-b7d8-475e08858a77",
   "metadata": {},
   "source": [
    "# Task B: Finding the top 10 brands from frequency counts.\n",
    "\n",
    "Note: Stopwords not counted, and analysis is performed at the brand level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc8c9b-862f-4880-868c-4f409ff8d951",
   "metadata": {},
   "source": [
    "## Replacing car models with brands in the scraped data (without stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8fcce010-e56a-43d1-a0aa-ac22595989a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load car models and brands data\n",
    "df2 = pd.read_csv(\"car_models_and_brands.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "34774347-6093-4891-9ce5-3aa98e034d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing non-car brands from data\n",
    "to_drop = ['car', 'seat', 'problem']\n",
    "df2 = df2[~df2['Brand'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "472f11cc-c5a4-4f30-b527-6e52a118ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_brands = df2['Brand'].str.lower().unique()\n",
    "model_to_brand = dict(zip(df2['Model'].str.lower(), df2['Brand'].str.lower()))\n",
    "results = []\n",
    "\n",
    "# Extract car brands from posts\n",
    "for index, row in df.iterrows():\n",
    "    message = row['Message']\n",
    "\n",
    "    found_brands = set([brand for brand in car_brands if brand in message.lower()]) \n",
    "    # Bug - if we read in ScrapedDataMod.csv, the above line fails to execute. However, if we don't re-read it in after scapring, this code executes.\n",
    "    found_models = [model for model in model_to_brand if model in message.lower()]\n",
    "    found_brands.update([model_to_brand[model] for model in found_models])\n",
    "\n",
    "    results.append(', '.join(found_brands))\n",
    "\n",
    "df['Brands'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dd741d6f-b169-42ca-8d63-7627d71aebe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brands</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>1694</td>\n",
       "      <td>1694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acura</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acura, bmw, infiniti, audi, volkswagen</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acura, chevrolet, nissan, infiniti, toyota, volkswagen</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acura, dodge, infiniti, audi, bmw</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volkwagen, bmw</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volkwagen, buick</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volkwagen, sedan</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volkwagen, volkswagen</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volvo</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1008 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Message  Date\n",
       "Brands                                                           \n",
       "                                                       1694  1694\n",
       "acura                                                    38    38\n",
       "acura, bmw, infiniti, audi, volkswagen                    1     1\n",
       "acura, chevrolet, nissan, infiniti, toyota, vol...        1     1\n",
       "acura, dodge, infiniti, audi, bmw                         1     1\n",
       "...                                                     ...   ...\n",
       "volkwagen, bmw                                            1     1\n",
       "volkwagen, buick                                          1     1\n",
       "volkwagen, sedan                                          1     1\n",
       "volkwagen, volkswagen                                     1     1\n",
       "volvo                                                    27    27\n",
       "\n",
       "[1008 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by brands and count occurrences\n",
    "df.groupby('Brands').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b3f7b-a160-4491-bb41-34f48caf363f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70b5034e-7857-42e5-9cf1-041951580d14",
   "metadata": {},
   "source": [
    "# Task C: Calculating the lift ratios for associations between the top 10 brands identified in Task A.\n",
    "\n",
    "Note: Counting of a brand is done once per post, and a message is not counted in the lift calculations if the mentions of the two brands are separated by approximately 5-7 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2f543-6c84-4b50-ba96-64cfb61b9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lift values need to be in a table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ae44e-9793-4af1-b94a-df772e6a1a19",
   "metadata": {},
   "source": [
    "# Task D: Plotting the brands on a multi-dimensional scaling (MDS) map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e24f73-88c7-4a05-bc0e-0ab0103745d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c36daec-6a9c-4266-a8c5-65d92c34651f",
   "metadata": {},
   "source": [
    "# Task E: Insights from Tasks C and D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11355dc2-ce8b-4a9b-b560-710bb18a49b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a52caa76-8f23-4a61-b472-af3d57b91705",
   "metadata": {},
   "source": [
    "# Task F: Identifying the top 5 most frequently mentioned attributes of features of cars in the discussions and which attributes are strongly associated with which of the 5 brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d07844b7-9dba-4b85-99fd-2f111001cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to clarify wording with professor on wording of this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4259bf-8f97-46d9-9879-d8f16538611e",
   "metadata": {},
   "source": [
    "# Task G: Advice for Client based on Task F."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377463a-785c-4792-98ee-20589c446aca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e861b2e-8299-4cb3-b9ef-11245724699e",
   "metadata": {},
   "source": [
    "# Task H: Identifying the most aspirational brand in the data in terms of people wanting to actually buy or own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11094693-10a5-47ff-a4be-20b0cc6cd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe analyses\n",
    "    # Describe how we measured \"aspirational\" and how we found the most aspirational brand. \n",
    "# Describe business implications for this brand"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
